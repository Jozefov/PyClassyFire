{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyClassyFire Tutorial: Classifying Chemical Compounds Using the ClassyFire API\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the **PyClassyFire** tutorial! This guide will walk you through the process of classifying a large set of chemical compounds using the [ClassyFire](http://classyfire.wishartlab.com/) API. We'll utilize the `PyClassyFire` package, which provides a command-line interface (CLI) and programmatic access to the ClassyFire service, enabling efficient and scalable classification of chemical structures.\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Preprocess your SMILES data**: Prepare your unique SMILES strings for classification.\n",
    "2. **Submit classification jobs**: Use the `PyClassyFire` package to send your data to the ClassyFire API.\n",
    "3. **Retrieve and process results**: Collect the classification results and merge them with your original data.\n",
    "4. **Save the annotated data**: Store the enriched dataset for further analysis.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into the tutorial, ensure you have the following:\n",
    "\n",
    "- **Conda Environment**: A Conda environment named `classyfire_env` with all necessary dependencies installed.\n",
    "- **PyClassyFire Package**: Installed and accessible within your Conda environment.\n",
    "- **Unique SMILES Data**: A TSV file containing approximately 16,000 unique SMILES strings located at `/Users/macbook/CODE/PyClassyFire/data/unique_valid_smiles_no_header.tsv`.\n",
    "\n",
    "> **Note:** This tutorial assumes that the Conda environment and `PyClassyFire` package are already set up. If not, please refer to the [repository's README](https://github.com/yourusername/PyClassyFire) for setup instructions.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Importing Necessary Libraries](#importing-libraries)\n",
    "2. [Loading and Exploring the Data](#loading-data)\n",
    "3. [Preparing the SMILES Data for Classification](#preparing-data)\n",
    "4. [Submitting Classification Jobs to ClassyFire API](#submitting-jobs)\n",
    "5. [Monitoring Job Progress](#monitoring-progress)\n",
    "6. [Retrieving and Processing Results](#retrieving-results)\n",
    "7. [Saving the Annotated Data](#saving-data)\n",
    "8. [Conclusion](#conclusion)\n"
   ],
   "id": "82b5bab971f2733b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:38.603640Z",
     "start_time": "2025-01-01T22:36:38.179608Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from classyfire_cli.src.utils import MoleCule, load_existing_results, save_intermediate_results\n",
    "from classyfire_cli.src.batch import process_batches_with_saving_and_retry"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:38.719514Z",
     "start_time": "2025-01-01T22:36:38.717606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "smiles_file_path = '../data/unique_valid_smiles_no_header.tsv'\n",
    "output_dir = '../data/intermediate_results/'\n",
    "final_output_path = '/Users/macbook/CODE/PyClassyFire/data/classified_smiles.tsv'"
   ],
   "id": "6545871a47e06cba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:41.407570Z",
     "start_time": "2025-01-01T22:36:39.492043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load SMILES data\n",
    "smiles_df = pd.read_csv(smiles_file_path, sep='\\t', header=None, names=['SMILES']).dropna()\n",
    "\n",
    "# Canonicalize SMILES\n",
    "smiles_df['Canonical_SMILES'] = smiles_df['SMILES'].apply(\n",
    "    lambda x: MoleCule.from_smiles(x).canonical_smiles if x else None\n",
    ").dropna()"
   ],
   "id": "afbafdb7a69c2e6a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:41.421167Z",
     "start_time": "2025-01-01T22:36:41.417870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove invalid entries\n",
    "invalid_smiles = smiles_df['Canonical_SMILES'].isnull().sum()\n",
    "print(f\"Number of invalid SMILES after canonicalization: {invalid_smiles}\")\n",
    "\n",
    "if invalid_smiles > 0:\n",
    "    smiles_df = smiles_df.dropna(subset=['Canonical_SMILES'])\n",
    "    print(f\"Removed {invalid_smiles} invalid SMILES entries.\")"
   ],
   "id": "54e7162949f396e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid SMILES after canonicalization: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:42.321905Z",
     "start_time": "2025-01-01T22:36:42.319015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset index after cleaning\n",
    "smiles_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Extract the list of canonical SMILES\n",
    "canonical_smiles_list = smiles_df['Canonical_SMILES'].tolist()"
   ],
   "id": "ddd2aeb08beeb1e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:42.585Z",
     "start_time": "2025-01-01T22:36:42.581938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "batch_size = 100          # Number of SMILES per job\n",
    "save_interval = 20        # Save intermediate results every 20 batches\n",
    "output_dir = '/Users/macbook/CODE/PyClassyFire/data/intermediate_results/'\n",
    "max_retries = 3           # Maximum number of retries for failed batches\n",
    "retry_delay = 10         # Delay between retries in seconds (5 minutes)"
   ],
   "id": "2e2d3d4f8b60398a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-01T22:36:59.198637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process the batches with resumption and retry logic\n",
    "intermediate_files = process_batches_with_saving_and_retry(\n",
    "    smiles_list=canonical_smiles_list,\n",
    "    batch_size=batch_size,\n",
    "    output_dir=output_dir,\n",
    "    max_retries=max_retries,\n",
    "    retry_delay=retry_delay\n",
    ")"
   ],
   "id": "1dc92d621e1c6491",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed SMILES: 0\n",
      "0 13984\n",
      "Remaining SMILES to process: 13984\n",
      "Remaining unique SMILES to process after removing duplicates: 13984\n",
      "Total remaining batches to process: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted Batch 1 with Query ID 12021424\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:36:56.708358Z",
     "start_time": "2025-01-01T22:36:56.703819Z"
    }
   },
   "cell_type": "code",
   "source": "len(set(canonical_smiles_list))",
   "id": "b0886000b2169998",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13984"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_intermediate_results(intermediate_files):\n",
    "    \"\"\"\n",
    "    Merges multiple intermediate JSON result files into a single dictionary.\n",
    "    \"\"\"\n",
    "    merged_results = {}\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                merged_results.update(data)\n",
    "            print(f\"Successfully merged results from {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging results from {file}: {e}\")\n",
    "    return merged_results"
   ],
   "id": "dc9bdca23fccf99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge all intermediate results\n",
    "merged_results = merge_intermediate_results(intermediate_files)"
   ],
   "id": "3e46aa4945742ec7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display the number of classified SMILES\n",
    "classified_count = len(merged_results)\n",
    "print(f\"Total number of classified SMILES: {classified_count}\")\n",
    "\n",
    "# Convert the merged results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame.from_dict(merged_results, orient='index')\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'Canonical_SMILES'}, inplace=True)\n",
    "\n",
    "# Merge the classification results with the original SMILES DataFrame\n",
    "annotated_df = pd.merge(smiles_df, results_df, on='Canonical_SMILES', how='left')"
   ],
   "id": "55114f2112ee54c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Handle unclassified SMILES\n",
    "unclassified = annotated_df['superclass'].isnull().sum()\n",
    "print(f\"Number of SMILES without classification: {unclassified}\")\n",
    "\n",
    "# Fill NaN values with 'Unknown'\n",
    "annotated_df[['superclass', 'class', 'subclass']] = annotated_df[['superclass', 'class', 'subclass']].fillna('Unknown')\n",
    "\n",
    "# Save the annotated DataFrame to a TSV file\n",
    "annotated_df.to_csv(final_output_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Annotated data has been saved to {final_output_path}\")"
   ],
   "id": "67ae5471c400216d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28e7469ee2118b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77ad7ac4166aaa1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T20:51:33.375005Z",
     "start_time": "2025-01-01T20:51:33.371444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_structure(data, level=0):\n",
    "    \"\"\"Recursively analyze and print the structure of JSON data.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        print(\" \" * level + f\"Object with keys: {list(data.keys())}\")\n",
    "        for key, value in data.items():\n",
    "            analyze_structure(value, level + 2)\n",
    "    elif isinstance(data, list):\n",
    "        print(\" \" * level + f\"List of {len(data)} items\")\n",
    "        if len(data) > 0:\n",
    "            analyze_structure(data[0], level + 2)  # Analyze the first item as representative\n",
    "    else:\n",
    "        print(\" \" * level + f\"Value type: {type(data).__name__}\")\n",
    "\n"
   ],
   "id": "afdc3fa695ec8852",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T20:51:36.018760Z",
     "start_time": "2025-01-01T20:51:36.010196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the JSON file\n",
    "file_path = \"/Users/macbook/CODE/PyClassyFire/data/intermediate_results/intermediate_1.json\"  # Replace with your file's path\n",
    "with open(file_path, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Analyze the JSON structure\n",
    "analyze_structure(json_data)"
   ],
   "id": "2550c36dbcfa512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with keys: ['12021409']\n",
      "  List of 100 items\n",
      "    Object with keys: ['identifier', 'smiles', 'inchikey', 'kingdom', 'superclass', 'class', 'subclass', 'intermediate_nodes', 'direct_parent', 'alternative_parents', 'molecular_framework', 'substituents', 'description', 'external_descriptors', 'ancestors', 'predicted_chebi_terms', 'predicted_lipidmaps_terms', 'classification_version']\n",
      "      Value type: str\n",
      "      Value type: str\n",
      "      Value type: str\n",
      "      Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "      Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "      Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "      Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "      List of 0 items\n",
      "      Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "        Value type: str\n",
      "      List of 27 items\n",
      "        Object with keys: ['name', 'description', 'chemont_id', 'url']\n",
      "          Value type: str\n",
      "          Value type: str\n",
      "          Value type: str\n",
      "          Value type: str\n",
      "      Value type: str\n",
      "      List of 43 items\n",
      "        Value type: str\n",
      "      Value type: str\n",
      "      List of 0 items\n",
      "      List of 51 items\n",
      "        Value type: str\n",
      "      List of 36 items\n",
      "        Value type: str\n",
      "      List of 1 items\n",
      "        Value type: str\n",
      "      Value type: str\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T22:01:58.075785Z",
     "start_time": "2025-01-01T22:01:58.068533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "existing_smiles = load_existing_results(output_dir)\n",
    "print(f\"Already processed SMILES: {len(existing_smiles[0])}\")"
   ],
   "id": "b8b3148522350d69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed SMILES: 100\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c1d582c0a0f1d31f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b6db6fdd1d01d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:45:55.183266Z",
     "start_time": "2025-01-01T16:45:55.175544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_batches_with_saving_and_retry(\n",
    "    smiles_list,\n",
    "    batch_size=100,\n",
    "    save_interval=20,\n",
    "    output_dir='../data/intermediate_results/',\n",
    "    max_retries=3,\n",
    "    retry_delay=300  # in seconds (5 minutes)\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes SMILES in batches, submits them to the ClassyFire API, saves intermediate results,\n",
    "    and implements retry logic for failed batches.\n",
    "    \n",
    "    Parameters:\n",
    "    - smiles_list (list): List of canonical SMILES strings to classify.\n",
    "    - batch_size (int): Number of SMILES per batch/job.\n",
    "    - save_interval (int): Save intermediate results every 'save_interval' batches.\n",
    "    - output_dir (str): Directory to save intermediate result files.\n",
    "    - max_retries (int): Maximum number of retries for failed batches.\n",
    "    - retry_delay (int): Delay between retries in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - saved_files (list): List of paths to the saved intermediate JSON files.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Split the SMILES list into batches\n",
    "    batches = chunk_tasks(smiles_list, batch_size)\n",
    "    total_batches = len(batches)\n",
    "    print(f\"Total number of batches to process: {total_batches}\")\n",
    "    logging.info(f\"Total number of batches to process: {total_batches}\")\n",
    "    \n",
    "    # Create Job instances for each batch\n",
    "    jobs = [Job(batch) for batch in batches]\n",
    "    \n",
    "    # Initialize Scheduler\n",
    "    scheduler = Scheduler(jobs)\n",
    "    \n",
    "    # Initialize counters\n",
    "    processed_batches = 0\n",
    "    saved_files = []\n",
    "    current_batch = 0\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_batches, desc=\"Processing Batches\")\n",
    "    \n",
    "    while scheduler.jobs:\n",
    "        try:\n",
    "            job = scheduler.jobs[0]\n",
    "            if job.query_id is None:\n",
    "                # Submit the job\n",
    "                job.submit()\n",
    "                logging.info(f\"Submitted Job {current_batch + 1}/{total_batches} with Query ID {job.query_id}\")\n",
    "                print(f\"Submitted Job {current_batch + 1}/{total_batches} with Query ID {job.query_id}\")\n",
    "                time.sleep(60)  # Wait before checking status\n",
    "            elif job.is_done:\n",
    "                # Parse and store the results\n",
    "                scheduler.results[job.query_id] = job.parse_results()\n",
    "                logging.info(f\"Job {current_batch + 1}/{total_batches} completed.\")\n",
    "                print(f\"Job {current_batch + 1}/{total_batches} completed.\")\n",
    "                scheduler.jobs.pop(0)\n",
    "                processed_batches += 1\n",
    "                current_batch += 1\n",
    "                pbar.update(1)\n",
    "                time.sleep(10)  # Short wait before processing next job\n",
    "                \n",
    "                # Save intermediate results every 'save_interval' batches\n",
    "                if processed_batches % save_interval == 0:\n",
    "                    intermediate_file = os.path.join(output_dir, f'intermediate_{processed_batches}.json')\n",
    "                    with open(intermediate_file, 'w') as f:\n",
    "                        json.dump(scheduler.results, f, indent=4)\n",
    "                    saved_files.append(intermediate_file)\n",
    "                    logging.info(f\"Saved intermediate results to {intermediate_file}\")\n",
    "                    print(f\"Saved intermediate results to {intermediate_file}\")\n",
    "            elif job.is_stale:\n",
    "                # Handle stale jobs\n",
    "                logging.warning(f\"Job {current_batch + 1}/{total_batches} is stale. Skipping.\")\n",
    "                print(f\"Job {current_batch + 1}/{total_batches} is stale. Skipping.\")\n",
    "                scheduler.results[job.query_id] = []\n",
    "                scheduler.jobs.pop(0)\n",
    "                processed_batches += 1\n",
    "                current_batch += 1\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                # Job is still running\n",
    "                time.sleep(60)  # Wait before rechecking\n",
    "        except urllib.error.HTTPError as e:\n",
    "            logging.error(f\"HTTPError encountered: {e}. Retrying Job {current_batch + 1}/{total_batches}.\")\n",
    "            print(f\"HTTPError encountered: {e}. Retrying Job {current_batch + 1}/{total_batches}.\")\n",
    "            if scheduler.retry < max_retries:\n",
    "                scheduler.retry += 1\n",
    "                logging.info(f\"Retrying Job {current_batch + 1}/{total_batches} after {retry_delay} seconds.\")\n",
    "                print(f\"Retrying Job {current_batch + 1}/{total_batches} after {retry_delay} seconds.\")\n",
    "                time.sleep(retry_delay)  # Wait before retrying\n",
    "            else:\n",
    "                logging.error(f\"Maximum retries reached for Job {current_batch + 1}/{total_batches}. Skipping.\")\n",
    "                print(f\"Maximum retries reached for Job {current_batch + 1}/{total_batches}. Skipping.\")\n",
    "                scheduler.jobs.pop(0)\n",
    "                scheduler.results[job.query_id] = []\n",
    "                processed_batches += 1\n",
    "                current_batch += 1\n",
    "                pbar.update(1)\n",
    "                scheduler.retry = 0  # Reset retry counter\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}. Skipping Job {current_batch + 1}/{total_batches}.\")\n",
    "            print(f\"Unexpected error: {e}. Skipping Job {current_batch + 1}/{total_batches}.\")\n",
    "            scheduler.jobs.pop(0)\n",
    "            scheduler.results[job.query_id] = []\n",
    "            processed_batches += 1\n",
    "            current_batch += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save any remaining results after processing all jobs\n",
    "    if processed_batches % save_interval != 0:\n",
    "        intermediate_file = os.path.join(output_dir, f'intermediate_{processed_batches}.json')\n",
    "        with open(intermediate_file, 'w') as f:\n",
    "            json.dump(scheduler.results, f, indent=4)\n",
    "        saved_files.append(intermediate_file)\n",
    "        logging.info(f\"Saved final intermediate results to {intermediate_file}\")\n",
    "        print(f\"Saved final intermediate results to {intermediate_file}\")\n",
    "    \n",
    "    return saved_files"
   ],
   "id": "d7eebe14734d8865",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_existing_results(output_dir):\n",
    "    \"\"\"\n",
    "    Loads existing intermediate JSON files and returns a set of already processed SMILES.\n",
    "    \"\"\"\n",
    "    merged_results = {}\n",
    "    if not os.path.exists(output_dir):\n",
    "        return merged_results\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.startswith('intermediate_') and file.endswith('.json'):\n",
    "            with open(os.path.join(output_dir, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                merged_results.update(data)\n",
    "    return merged_results"
   ],
   "id": "6061ede26d6bae07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bbe1fefbb2996c26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d1308c683969d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a779e2d87ec4169b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T19:45:11.108363Z",
     "start_time": "2025-01-01T16:45:55.381412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the list of canonical SMILES\n",
    "canonical_smiles_list = smiles_df['Canonical_SMILES'].tolist()\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 100          # Number of SMILES per job\n",
    "save_interval = 20        # Save intermediate results every 20 batches\n",
    "output_dir = '/Users/macbook/CODE/PyClassyFire/data/intermediate_results/'\n",
    "max_retries = 3           # Maximum number of retries for failed batches\n",
    "retry_delay = 300         # Delay between retries in seconds (5 minutes)\n",
    "\n",
    "# Process the batches and save intermediate results with retry logic\n",
    "intermediate_files = process_batches_with_saving_and_retry(\n",
    "    smiles_list=canonical_smiles_list,\n",
    "    batch_size=batch_size,\n",
    "    save_interval=save_interval,\n",
    "    output_dir=output_dir,\n",
    "    max_retries=max_retries,\n",
    "    retry_delay=retry_delay\n",
    ")"
   ],
   "id": "9bb47ffa7a4b9dea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches to process: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted Job 1/140 with Query ID 12021290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   1%|          | 1/140 [01:49<4:13:04, 109.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 1/140 completed.\n",
      "Submitted Job 2/140 with Query ID 12021291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   1%|▏         | 2/140 [03:52<4:29:52, 117.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 2/140 completed.\n",
      "Submitted Job 3/140 with Query ID 12021292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   2%|▏         | 3/140 [05:49<4:28:16, 117.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 3/140 completed.\n",
      "Submitted Job 4/140 with Query ID 12021293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   3%|▎         | 4/140 [07:49<4:27:57, 118.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 4/140 completed.\n",
      "Submitted Job 5/140 with Query ID 12021294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   4%|▎         | 5/140 [09:46<4:25:21, 117.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 5/140 completed.\n",
      "Submitted Job 6/140 with Query ID 12021295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   4%|▍         | 6/140 [11:44<4:23:34, 118.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 6/140 completed.\n",
      "Submitted Job 7/140 with Query ID 12021296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   5%|▌         | 7/140 [13:53<4:29:34, 121.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 7/140 completed.\n",
      "Submitted Job 8/140 with Query ID 12021297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   6%|▌         | 8/140 [16:06<4:35:05, 125.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 8/140 completed.\n",
      "Submitted Job 9/140 with Query ID 12021298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   6%|▋         | 9/140 [18:16<4:36:43, 126.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 9/140 completed.\n",
      "Submitted Job 10/140 with Query ID 12021299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   7%|▋         | 10/140 [20:30<4:39:13, 128.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 10/140 completed.\n",
      "Submitted Job 11/140 with Query ID 12021300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   8%|▊         | 11/140 [22:41<4:38:38, 129.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 11/140 completed.\n",
      "Submitted Job 12/140 with Query ID 12021301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   9%|▊         | 12/140 [24:56<4:39:42, 131.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 12/140 completed.\n",
      "Submitted Job 13/140 with Query ID 12021302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   9%|▉         | 13/140 [27:08<4:38:26, 131.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 13/140 completed.\n",
      "Submitted Job 14/140 with Query ID 12021303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  10%|█         | 14/140 [29:18<4:34:52, 130.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 14/140 completed.\n",
      "Submitted Job 15/140 with Query ID 12021304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  11%|█         | 15/140 [31:25<4:30:15, 129.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 15/140 completed.\n",
      "Submitted Job 16/140 with Query ID 12021305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  11%|█▏        | 16/140 [33:32<4:26:20, 128.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 16/140 completed.\n",
      "Submitted Job 17/140 with Query ID 12021306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  12%|█▏        | 17/140 [35:42<4:25:14, 129.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 17/140 completed.\n",
      "Submitted Job 18/140 with Query ID 12021308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  13%|█▎        | 18/140 [38:26<4:43:51, 139.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 18/140 completed.\n",
      "Submitted Job 19/140 with Query ID 12021309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  14%|█▎        | 19/140 [40:38<4:37:23, 137.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021309.json. Skipping Job 19/140.\n",
      "Submitted Job 20/140 with Query ID 12021311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  14%|█▍        | 20/140 [42:55<4:34:30, 137.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 20/140 completed.\n",
      "Saved intermediate results to /Users/macbook/CODE/PyClassyFire/data/intermediate_results/intermediate_20.json\n",
      "Submitted Job 21/140 with Query ID 12021312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  15%|█▌        | 21/140 [45:08<4:30:01, 136.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021312.json. Skipping Job 21/140.\n",
      "Submitted Job 22/140 with Query ID 12021313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  16%|█▌        | 22/140 [47:13<4:20:56, 132.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021313.json. Skipping Job 22/140.\n",
      "Submitted Job 23/140 with Query ID 12021314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  16%|█▋        | 23/140 [49:34<4:23:38, 135.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 23/140 completed.\n",
      "Submitted Job 24/140 with Query ID 12021316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  17%|█▋        | 24/140 [52:05<4:30:34, 139.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 24/140 completed.\n",
      "Submitted Job 25/140 with Query ID 12021319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  18%|█▊        | 25/140 [54:28<4:30:04, 140.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 25/140 completed.\n",
      "Submitted Job 26/140 with Query ID 12021322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  19%|█▊        | 26/140 [57:04<4:35:57, 145.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 26/140 completed.\n",
      "Submitted Job 27/140 with Query ID 12021325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  19%|█▉        | 27/140 [59:44<4:41:59, 149.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 27/140 completed.\n",
      "Submitted Job 28/140 with Query ID 12021328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  20%|██        | 28/140 [1:01:56<4:29:33, 144.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021328.json. Skipping Job 28/140.\n",
      "Submitted Job 29/140 with Query ID 12021331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  21%|██        | 29/140 [1:04:01<4:16:44, 138.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021331.json. Skipping Job 29/140.\n",
      "Submitted Job 30/140 with Query ID 12021333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  21%|██▏       | 30/140 [1:06:25<4:16:51, 140.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 30/140 completed.\n",
      "Submitted Job 31/140 with Query ID 12021337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  22%|██▏       | 31/140 [1:08:39<4:11:21, 138.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021337.json. Skipping Job 31/140.\n",
      "Submitted Job 32/140 with Query ID 12021338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  23%|██▎       | 32/140 [1:11:04<4:12:23, 140.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 32/140 completed.\n",
      "Submitted Job 33/140 with Query ID 12021341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  24%|██▎       | 33/140 [1:13:16<4:06:00, 137.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021341.json. Skipping Job 33/140.\n",
      "Submitted Job 34/140 with Query ID 12021343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  24%|██▍       | 34/140 [1:15:20<3:55:59, 133.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021343.json. Skipping Job 34/140.\n",
      "Submitted Job 35/140 with Query ID 12021345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  25%|██▌       | 35/140 [1:17:21<3:47:33, 130.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021345.json. Skipping Job 35/140.\n",
      "Submitted Job 36/140 with Query ID 12021346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  26%|██▌       | 36/140 [1:19:24<3:41:33, 127.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021346.json. Skipping Job 36/140.\n",
      "Submitted Job 37/140 with Query ID 12021349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  26%|██▋       | 37/140 [1:21:29<3:38:09, 127.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021349.json. Skipping Job 37/140.\n",
      "Submitted Job 38/140 with Query ID 12021353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  27%|██▋       | 38/140 [1:23:58<3:47:11, 133.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 38/140 completed.\n",
      "Submitted Job 39/140 with Query ID 12021356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  28%|██▊       | 39/140 [1:26:31<3:54:48, 139.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 39/140 completed.\n",
      "Submitted Job 40/140 with Query ID 12021359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  29%|██▊       | 40/140 [1:28:44<3:49:08, 137.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021359.json. Skipping Job 40/140.\n",
      "Submitted Job 41/140 with Query ID 12021362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  29%|██▉       | 41/140 [1:30:47<3:39:21, 132.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021362.json. Skipping Job 41/140.\n",
      "Submitted Job 42/140 with Query ID 12021365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  30%|███       | 42/140 [1:33:15<3:44:51, 137.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 42/140 completed.\n",
      "Submitted Job 43/140 with Query ID 12021371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  31%|███       | 43/140 [1:35:32<3:41:56, 137.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 43/140 completed.\n",
      "Submitted Job 44/140 with Query ID 12021376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  31%|███▏      | 44/140 [1:38:04<3:46:52, 141.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 44/140 completed.\n",
      "Submitted Job 45/140 with Query ID 12021377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  32%|███▏      | 45/140 [1:40:40<3:51:14, 146.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 45/140 completed.\n",
      "Submitted Job 46/140 with Query ID 12021378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  33%|███▎      | 46/140 [1:42:58<3:45:08, 143.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 46/140 completed.\n",
      "Submitted Job 47/140 with Query ID 12021379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  34%|███▎      | 47/140 [1:45:26<3:44:27, 144.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 47/140 completed.\n",
      "Submitted Job 48/140 with Query ID 12021380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  34%|███▍      | 48/140 [1:48:00<3:46:18, 147.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 48/140 completed.\n",
      "Submitted Job 49/140 with Query ID 12021381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  35%|███▌      | 49/140 [1:49:36<3:20:36, 132.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 500 Server Error: Internal Server Error for url: http://classyfire.wishartlab.com/queries/12021381.json. Skipping Job 49/140.\n",
      "Submitted Job 50/140 with Query ID 12021382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  36%|███▌      | 50/140 [1:51:57<3:22:20, 134.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 50/140 completed.\n",
      "Submitted Job 51/140 with Query ID 12021383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  36%|███▋      | 51/140 [1:53:50<3:10:25, 128.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 500 Server Error: Internal Server Error for url: http://classyfire.wishartlab.com/queries/12021383.json. Skipping Job 51/140.\n",
      "Submitted Job 52/140 with Query ID 12021384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  37%|███▋      | 52/140 [1:55:50<3:04:18, 125.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 52/140 completed.\n",
      "Submitted Job 53/140 with Query ID 12021386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  38%|███▊      | 53/140 [1:58:36<3:19:45, 137.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 53/140 completed.\n",
      "Submitted Job 54/140 with Query ID 12021388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  39%|███▊      | 54/140 [2:00:53<3:17:04, 137.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021388.json. Skipping Job 54/140.\n",
      "Submitted Job 55/140 with Query ID 12021389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  39%|███▉      | 55/140 [2:03:27<3:21:50, 142.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 55/140 completed.\n",
      "Submitted Job 56/140 with Query ID 12021390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  40%|████      | 56/140 [2:05:40<3:15:45, 139.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021390.json. Skipping Job 56/140.\n",
      "Submitted Job 57/140 with Query ID 12021391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  41%|████      | 57/140 [2:07:15<2:54:51, 126.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 500 Server Error: Internal Server Error for url: http://classyfire.wishartlab.com/queries/12021391.json. Skipping Job 57/140.\n",
      "Submitted Job 58/140 with Query ID 12021392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  41%|████▏     | 58/140 [2:09:18<2:51:11, 125.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021392.json. Skipping Job 58/140.\n",
      "Submitted Job 59/140 with Query ID 12021393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  42%|████▏     | 59/140 [2:11:20<2:47:43, 124.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021393.json. Skipping Job 59/140.\n",
      "Submitted Job 60/140 with Query ID 12021394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  43%|████▎     | 60/140 [2:13:23<2:45:18, 123.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021394.json. Skipping Job 60/140.\n",
      "Submitted Job 61/140 with Query ID 12021395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  44%|████▎     | 61/140 [2:15:28<2:43:37, 124.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021395.json. Skipping Job 61/140.\n",
      "Submitted Job 62/140 with Query ID 12021396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  44%|████▍     | 62/140 [2:17:33<2:41:50, 124.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021396.json. Skipping Job 62/140.\n",
      "Submitted Job 63/140 with Query ID 12021397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  45%|████▌     | 63/140 [2:19:35<2:38:52, 123.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021397.json. Skipping Job 63/140.\n",
      "Submitted Job 64/140 with Query ID 12021398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  46%|████▌     | 64/140 [2:22:09<2:48:20, 132.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 64/140 completed.\n",
      "Submitted Job 65/140 with Query ID 12021399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  46%|████▋     | 65/140 [2:24:23<2:46:18, 133.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021399.json. Skipping Job 65/140.\n",
      "Submitted Job 66/140 with Query ID 12021400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  47%|████▋     | 66/140 [2:26:53<2:50:21, 138.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 66/140 completed.\n",
      "Submitted Job 67/140 with Query ID 12021401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  48%|████▊     | 67/140 [2:29:31<2:55:14, 144.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 67/140 completed.\n",
      "Submitted Job 68/140 with Query ID 12021402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  49%|████▊     | 68/140 [2:32:12<2:59:06, 149.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 68/140 completed.\n",
      "Submitted Job 69/140 with Query ID 12021403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  49%|████▉     | 69/140 [2:34:25<2:50:56, 144.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021403.json. Skipping Job 69/140.\n",
      "Submitted Job 70/140 with Query ID 12021404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  50%|█████     | 70/140 [2:36:33<2:42:44, 139.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021404.json. Skipping Job 70/140.\n",
      "Submitted Job 71/140 with Query ID 12021405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  51%|█████     | 71/140 [2:38:38<2:35:20, 135.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: 504 Server Error: Gateway Time-out for url: http://classyfire.wishartlab.com/queries/12021405.json. Skipping Job 71/140.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  51%|█████▏    | 72/140 [2:39:51<2:12:04, 116.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Skipping Job 72/140.\n",
      "Submitted Job 73/140 with Query ID 12021406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m retry_delay \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m         \u001B[38;5;66;03m# Delay between retries in seconds (5 minutes)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Process the batches and save intermediate results with retry logic\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m intermediate_files \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_batches_with_saving_and_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43msmiles_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcanonical_smiles_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry_delay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_delay\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 56\u001B[0m, in \u001B[0;36mprocess_batches_with_saving_and_retry\u001B[0;34m(smiles_list, batch_size, save_interval, output_dir, max_retries, retry_delay)\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSubmitted Job \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_batch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with Query ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjob\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     55\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m60\u001B[39m)  \u001B[38;5;66;03m# Wait before checking status\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_done\u001B[49m:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Parse and store the results\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mresults[job\u001B[38;5;241m.\u001B[39mquery_id] \u001B[38;5;241m=\u001B[39m job\u001B[38;5;241m.\u001B[39mparse_results()\n\u001B[1;32m     59\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJob \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_batch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m completed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/CODE/PyClassyFire/classyfire_cli/src/batch.py:22\u001B[0m, in \u001B[0;36mJob.is_done\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_done\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m---> 22\u001B[0m     result \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_id\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification_status\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDone\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/CODE/PyClassyFire/classyfire_cli/src/api.py:46\u001B[0m, in \u001B[0;36mget_results\u001B[0;34m(query_id, return_format)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_results\u001B[39m(query_id, return_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     32\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Given a query_id, fetch the classification results.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m    :param query_id: A numeric query id returned at time of query submission\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m/queries/\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_format\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mContent-Type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mapplication/\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mreturn_format\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     r\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m r\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/sessions.py:746\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m--> 746\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/models.py:902\u001B[0m, in \u001B[0;36mResponse.content\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 902\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/response.py:1063\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;124;03m    'content-encoding' header.\u001B[39;00m\n\u001B[1;32m   1061\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1062\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupports_chunked_reads():\n\u001B[0;32m-> 1063\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_chunked(amt, decode_content\u001B[38;5;241m=\u001B[39mdecode_content)\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1065\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/response.py:1222\u001B[0m, in \u001B[0;36mHTTPResponse.read_chunked\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1220\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1221\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1222\u001B[0m chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1223\u001B[0m decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode(\n\u001B[1;32m   1224\u001B[0m     chunk, decode_content\u001B[38;5;241m=\u001B[39mdecode_content, flush_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m )\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoded:\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/response.py:1168\u001B[0m, in \u001B[0;36mHTTPResponse._handle_chunk\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m   1166\u001B[0m     returned_chunk \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# amt > self.chunk_left\u001B[39;00m\n\u001B[0;32m-> 1168\u001B[0m     returned_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_safe_read\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_left\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[1;32m   1169\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39m_safe_read(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/http/client.py:631\u001B[0m, in \u001B[0;36mHTTPResponse._safe_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_safe_read\u001B[39m(\u001B[38;5;28mself\u001B[39m, amt):\n\u001B[1;32m    625\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read the number of bytes requested.\u001B[39;00m\n\u001B[1;32m    626\u001B[0m \n\u001B[1;32m    627\u001B[0m \u001B[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001B[39;00m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001B[39;00m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001B[39;00m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m<\u001B[39m amt:\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m IncompleteRead(data, amt\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(data))\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/socket.py:717\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 717\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_intermediate_results(intermediate_files):\n",
    "    \"\"\"\n",
    "    Merges multiple intermediate JSON result files into a single dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - intermediate_files (list): List of file paths to intermediate JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - merged_results (dict): Merged classification results.\n",
    "    \"\"\"\n",
    "    merged_results = {}\n",
    "    for file in intermediate_files:\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                merged_results.update(data)\n",
    "            logging.info(f\"Successfully merged results from {file}\")\n",
    "            print(f\"Successfully merged results from {file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error merging results from {file}: {e}\")\n",
    "            print(f\"Error merging results from {file}: {e}\")\n",
    "    return merged_results"
   ],
   "id": "b11b30d762792b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge all intermediate results\n",
    "merged_results = merge_intermediate_results(intermediate_files)\n",
    "\n",
    "# Display the number of classified SMILES\n",
    "classified_count = len(merged_results)\n",
    "print(f\"Total number of classified SMILES: {classified_count}\")"
   ],
   "id": "a00621491e034c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the merged results dictionary to a DataFrame\n",
    "# The dictionary keys are canonical SMILES, and values are classification details\n",
    "results_df = pd.DataFrame.from_dict(merged_results, orient='index')\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'Canonical_SMILES'}, inplace=True)\n",
    "\n",
    "# Display the first few entries of the results\n",
    "results_df.head()"
   ],
   "id": "8493792452e163a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the classification results with the original SMILES DataFrame\n",
    "annotated_df = pd.merge(smiles_df, results_df, on='Canonical_SMILES', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "annotated_df.head()"
   ],
   "id": "65ea0ae0b173b7d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for any SMILES that did not receive a classification\n",
    "unclassified = annotated_df['superclass'].isnull().sum()\n",
    "print(f\"Number of SMILES without classification: {unclassified}\")\n",
    "\n",
    "# Optionally, handle unclassified SMILES (e.g., mark as 'Unknown')\n",
    "annotated_df['superclass'].fillna('Unknown', inplace=True)\n",
    "annotated_df['class'].fillna('Unknown', inplace=True)\n",
    "annotated_df['subclass'].fillna('Unknown', inplace=True)"
   ],
   "id": "2bca5b8bdfa9e77f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the output path for the annotated data\n",
    "final_output_path = '/Users/macbook/CODE/PyClassyFire/data/classified_smiles.tsv'\n",
    "\n",
    "# Save the annotated DataFrame to a TSV file\n",
    "annotated_df.to_csv(final_output_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Annotated data has been saved to {final_output_path}\")"
   ],
   "id": "d40de108bff0d174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "526277c51edb36ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "519532d392eb86e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d6e2ab44ef34e26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7975a504b0bde0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:17:29.299894Z",
     "start_time": "2025-01-01T16:17:29.296084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the list of canonical SMILES\n",
    "canonical_smiles_list = smiles_df['Canonical_SMILES'].tolist()\n",
    "\n",
    "# Define the batch size (number of SMILES per job)\n",
    "batch_size = 100  # Adjust based on API limitations and performance\n",
    "\n",
    "# Create batches using the chunk_tasks utility function\n",
    "batches = chunk_tasks(canonical_smiles_list, batch_size)\n",
    "\n",
    "# Create Job instances for each batch\n",
    "jobs = [Job(batch) for batch in batches]\n",
    "\n",
    "print(f\"Total number of jobs created: {len(jobs)}\")"
   ],
   "id": "d33bd99300af4cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs created: 140\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:17:45.254568Z",
     "start_time": "2025-01-01T16:17:45.252077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Scheduler with the list of jobs\n",
    "scheduler = Scheduler(jobs)"
   ],
   "id": "d77ec92b787d77b1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:34:53.840648Z",
     "start_time": "2025-01-01T16:17:54.688728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start the classification process\n",
    "print(\"Submitting classification jobs to the ClassyFire API...\")\n",
    "scheduler.run()\n",
    "print(\"All jobs have been processed.\")"
   ],
   "id": "5a5386e4a1093fa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting classification jobs to the ClassyFire API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/140 [14:58<5:26:44, 146.30s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start the classification process\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSubmitting classification jobs to the ClassyFire API...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll jobs have been processed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/CODE/PyClassyFire/classyfire_cli/src/batch.py:54\u001B[0m, in \u001B[0;36mScheduler.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m job\u001B[38;5;241m.\u001B[39mis_done:\n\u001B[1;32m     53\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults[job\u001B[38;5;241m.\u001B[39mquery_id] \u001B[38;5;241m=\u001B[39m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjobs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     56\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/CODE/PyClassyFire/classyfire_cli/src/batch.py:30\u001B[0m, in \u001B[0;36mJob.parse_results\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_results\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m]:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_id\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentities\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/CODE/PyClassyFire/classyfire_cli/src/api.py:46\u001B[0m, in \u001B[0;36mget_results\u001B[0;34m(query_id, return_format)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_results\u001B[39m(query_id, return_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     32\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Given a query_id, fetch the classification results.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m    :param query_id: A numeric query id returned at time of query submission\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 46\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m/queries/\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_format\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mContent-Type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mapplication/\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mreturn_format\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     r\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m r\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    784\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    786\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 787\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    803\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    533\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 534\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/site-packages/urllib3/connection.py:516\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    513\u001B[0m _shutdown \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshutdown\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    515\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    519\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/UTILS/anaconda3/envs/classyfire_env/lib/python3.10/socket.py:717\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 717\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Export the results from the Scheduler\n",
    "classification_results = scheduler.export()\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "# The dictionary keys are canonical SMILES, and values are classification details\n",
    "results_df = pd.DataFrame.from_dict(classification_results, orient='index')\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'Canonical_SMILES'}, inplace=True)\n",
    "\n",
    "# Display the first few entries of the results\n",
    "results_df.head()"
   ],
   "id": "700262fd251dc3e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the classification results with the original SMILES DataFrame\n",
    "annotated_df = pd.merge(smiles_df, results_df, on='Canonical_SMILES', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "annotated_df.head()"
   ],
   "id": "d7e79d8d7777587d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for any SMILES that did not receive a classification\n",
    "unclassified = annotated_df['superclass'].isnull().sum()\n",
    "print(f\"Number of SMILES without classification: {unclassified}\")\n",
    "\n",
    "# Optionally, handle unclassified SMILES (e.g., mark as 'Unknown')\n",
    "annotated_df['superclass'].fillna('Unknown', inplace=True)\n",
    "annotated_df['class'].fillna('Unknown', inplace=True)\n",
    "annotated_df['subclass'].fillna('Unknown', inplace=True)"
   ],
   "id": "fbc3dd4baea4b52b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the output path for the annotated data\n",
    "output_file_path = '../data/classified_smiles.tsv'\n",
    "\n",
    "# Save the annotated DataFrame to a TSV file\n",
    "annotated_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Annotated data has been saved to {output_file_path}\")"
   ],
   "id": "1d2e7940b156e40e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c3749dd4dc6f47ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1258ae2af314c20d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
